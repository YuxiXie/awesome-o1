@ARTICLE{Zelikman2022-id,
  title         = "{STaR}: Bootstrapping reasoning with reasoning",
  author        = "Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah
                   D",
  journal       = "arXiv [cs.LG]",
  abstract      = "Generating step-by-step ``chain-of-thought'' rationales
                   improves language model performance on complex reasoning
                   tasks like mathematics or commonsense question-answering.
                   However, inducing language model rationale generation
                   currently requires either constructing massive rationale
                   datasets or sacrificing accuracy by using only few-shot
                   inference. We propose a technique to iteratively leverage a
                   small number of rationale examples and a large dataset
                   without rationales, to bootstrap the ability to perform
                   successively more complex reasoning. This technique, the
                   ``Self-Taught Reasoner'' (STaR), relies on a simple loop:
                   generate rationales to answer many questions, prompted with a
                   few rationale examples; if the generated answers are wrong,
                   try again to generate a rationale given the correct answer;
                   fine-tune on all the rationales that ultimately yielded
                   correct answers; repeat. We show that STaR significantly
                   improves performance on multiple datasets compared to a model
                   fine-tuned to directly predict final answers, and performs
                   comparably to fine-tuning a 30$\times$ larger
                   state-of-the-art language model on CommensenseQA. Thus, STaR
                   lets a model improve itself by learning from its own
                   generated reasoning.",
  month         =  "27~" # mar,
  year          =  2022,
  url           = "http://arxiv.org/abs/2203.14465",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Uesato2022-aw,
  title         = "Solving math word problems with process- and outcome-based
                   feedback",
  author        = "Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and
                   Song, Francis and Siegel, Noah and Wang, Lisa and Creswell,
                   Antonia and Irving, Geoffrey and Higgins, Irina",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent work has shown that asking language models to generate
                   reasoning steps improves performance on many reasoning tasks.
                   When moving beyond prompting, this raises the question of how
                   we should supervise such models: outcome-based approaches
                   which supervise the final result, or process-based approaches
                   which supervise the reasoning process itself? Differences
                   between these approaches might naturally be expected not just
                   in final-answer errors but also in reasoning errors, which
                   can be difficult to detect and are problematic in many
                   real-world domains such as education. We run the first
                   comprehensive comparison between process- and outcome-based
                   approaches trained on a natural language task, GSM8K. We find
                   that pure outcome-based supervision produces similar
                   final-answer error rates with less label supervision.
                   However, for correct reasoning steps we find it necessary to
                   use process-based supervision or supervision from learned
                   reward models that emulate process-based feedback. In total,
                   we improve the previous best results from 16.8\% $\to$ 12.7\%
                   final-answer error and 14.0\% $\to$ 3.4\% reasoning error
                   among final-answer-correct solutions.",
  month         =  "25~" # nov,
  year          =  2022,
  url           = "http://arxiv.org/abs/2211.14275",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Lightman2023-cr,
  title         = "Let's verify step by step",
  author        = "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and
                   Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan
                   and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
  journal       = "arXiv [cs.LG]",
  abstract      = "In recent years, large language models have greatly improved
                   in their ability to perform complex multi-step reasoning.
                   However, even state-of-the-art models still regularly produce
                   logical mistakes. To train more reliable models, we can turn
                   either to outcome supervision, which provides feedback for a
                   final result, or process supervision, which provides feedback
                   for each intermediate reasoning step. Given the importance of
                   training reliable models, and given the high cost of human
                   feedback, it is important to carefully compare the both
                   methods. Recent work has already begun this comparison, but
                   many questions still remain. We conduct our own
                   investigation, finding that process supervision significantly
                   outperforms outcome supervision for training models to solve
                   problems from the challenging MATH dataset. Our
                   process-supervised model solves 78\% of problems from a
                   representative subset of the MATH test set. Additionally, we
                   show that active learning significantly improves the efficacy
                   of process supervision. To support related research, we also
                   release PRM800K, the complete dataset of 800,000 step-level
                   human feedback labels used to train our best reward model.",
  month         =  "31~" # may,
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.20050",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Zelikman2024-cu,
  title         = "Quiet-{STaR}: Language models can teach themselves to think
                   before speaking",
  author        = "Zelikman, Eric and Harik, Georges and Shao, Yijia and
                   Jayasiri, Varuna and Haber, Nick and Goodman, Noah D",
  journal       = "arXiv [cs.CL]",
  abstract      = "When writing and talking, people sometimes pause to think.
                   Although reasoning-focused works have often framed reasoning
                   as a method of answering questions or completing agentic
                   tasks, reasoning is implicit in almost all written text. For
                   example, this applies to the steps not stated between the
                   lines of a proof or to the theory of mind underlying a
                   conversation. In the Self-Taught Reasoner (STaR, Zelikman et
                   al. 2022), useful thinking is learned by inferring rationales
                   from few-shot examples in question-answering and learning
                   from those that lead to a correct answer. This is a highly
                   constrained setting -- ideally, a language model could
                   instead learn to infer unstated rationales in arbitrary text.
                   We present Quiet-STaR, a generalization of STaR in which LMs
                   learn to generate rationales at each token to explain future
                   text, improving their predictions. We address key challenges,
                   including 1) the computational cost of generating
                   continuations, 2) the fact that the LM does not initially
                   know how to generate or use internal thoughts, and 3) the
                   need to predict beyond individual next tokens. To resolve
                   these, we propose a tokenwise parallel sampling algorithm,
                   using learnable tokens indicating a thought's start and end,
                   and an extended teacher-forcing technique. Encouragingly,
                   generated rationales disproportionately help model
                   difficult-to-predict tokens and improve the LM's ability to
                   directly answer difficult questions. In particular, after
                   continued pretraining of an LM on a corpus of internet text
                   with Quiet-STaR, we find zero-shot improvements on GSM8K
                   (5.9\%$\rightarrow$10.9\%) and CommonsenseQA
                   (36.3\%$\rightarrow$47.2\%) and observe a perplexity
                   improvement of difficult tokens in natural text. Crucially,
                   these improvements require no fine-tuning on these tasks.
                   Quiet-STaR marks a step towards LMs that can learn to reason
                   in a more general and scalable way.",
  month         =  "14~" # mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2403.09629",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  keywords      = "o1"
}

@ARTICLE{Gandhi2024-vs,
  title         = "Stream of search ({SoS}): Learning to search in language",
  author        = "Gandhi, Kanishk and Lee, Denise and Grand, Gabriel and Liu,
                   Muxin and Cheng, Winson and Sharma, Archit and Goodman, Noah
                   D",
  journal       = "arXiv [cs.LG]",
  abstract      = "Language models are rarely shown fruitful mistakes while
                   training. They then struggle to look beyond the next token,
                   suffering from a snowballing of errors and struggling to
                   predict the consequence of their actions several steps ahead.
                   In this paper, we show how language models can be taught to
                   search by representing the process of search in language, as
                   a flattened string -- a stream of search (SoS). We propose a
                   unified language for search that captures an array of
                   different symbolic search strategies. We demonstrate our
                   approach using the simple yet difficult game of Countdown,
                   where the goal is to combine input numbers with arithmetic
                   operations to reach a target number. We pretrain a
                   transformer-based language model from scratch on a dataset of
                   streams of search generated by heuristic solvers. We find
                   that SoS pretraining increases search accuracy by 25\% over
                   models trained to predict only the optimal search trajectory.
                   We further finetune this model with two policy improvement
                   methods: Advantage-Induced Policy Alignment (APA) and
                   Self-Taught Reasoner (STaR). The finetuned SoS models solve
                   36\% of previously unsolved problems, including problems that
                   cannot be solved by any of the heuristic solvers. Our results
                   indicate that language models can learn to solve problems via
                   search, self-improve to flexibly use different search
                   strategies, and potentially discover new ones.",
  month         =  "1~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.03683",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Kirchner2024-cu,
  title         = "Prover-Verifier Games improve legibility of {LLM} outputs",
  author        = "Kirchner, Jan Hendrik and Chen, Yining and Edwards, Harri and
                   Leike, Jan and McAleese, Nat and Burda, Yuri",
  journal       = "arXiv [cs.CL]",
  abstract      = "One way to increase confidence in the outputs of Large
                   Language Models (LLMs) is to support them with reasoning that
                   is clear and easy to check -- a property we call legibility.
                   We study legibility in the context of solving grade-school
                   math problems and show that optimizing chain-of-thought
                   solutions only for answer correctness can make them less
                   legible. To mitigate the loss in legibility, we propose a
                   training algorithm inspired by Prover-Verifier Game from Anil
                   et al. (2021). Our algorithm iteratively trains small
                   verifiers to predict solution correctness, ``helpful''
                   provers to produce correct solutions that the verifier
                   accepts, and ``sneaky'' provers to produce incorrect
                   solutions that fool the verifier. We find that the helpful
                   prover's accuracy and the verifier's robustness to
                   adversarial attacks increase over the course of training.
                   Furthermore, we show that legibility training transfers to
                   time-constrained humans tasked with verifying solution
                   correctness. Over course of LLM training human accuracy
                   increases when checking the helpful prover's solutions, and
                   decreases when checking the sneaky prover's solutions. Hence,
                   training for checkability by small verifiers is a plausible
                   technique for increasing output legibility. Our results
                   suggest legibility training against small verifiers as a
                   practical avenue for increasing legibility of large LLMs to
                   humans, and thus could help with alignment of superhuman
                   models.",
  month         =  "18~" # jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.13692",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  keywords      = "o1"
}

@ARTICLE{Snell2024-dx,
  title         = "Scaling {LLM} test-time compute optimally can be more
                   effective than scaling model parameters",
  author        = "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar,
                   Aviral",
  journal       = "arXiv [cs.LG]",
  abstract      = "Enabling LLMs to improve their outputs by using more
                   test-time computation is a critical step towards building
                   generally self-improving agents that can operate on
                   open-ended natural language. In this paper, we study the
                   scaling of inference-time computation in LLMs, with a focus
                   on answering the question: if an LLM is allowed to use a
                   fixed but non-trivial amount of inference-time compute, how
                   much can it improve its performance on a challenging prompt?
                   Answering this question has implications not only on the
                   achievable performance of LLMs, but also on the future of LLM
                   pretraining and how one should tradeoff inference-time and
                   pre-training compute. Despite its importance, little research
                   attempted to understand the scaling behaviors of various
                   test-time inference methods. Moreover, current work largely
                   provides negative results for a number of these strategies.
                   In this work, we analyze two primary mechanisms to scale
                   test-time computation: (1) searching against dense,
                   process-based verifier reward models; and (2) updating the
                   model's distribution over a response adaptively, given the
                   prompt at test time. We find that in both cases, the
                   effectiveness of different approaches to scaling test-time
                   compute critically varies depending on the difficulty of the
                   prompt. This observation motivates applying a
                   ``compute-optimal'' scaling strategy, which acts to most
                   effectively allocate test-time compute adaptively per prompt.
                   Using this compute-optimal strategy, we can improve the
                   efficiency of test-time compute scaling by more than 4x
                   compared to a best-of-N baseline. Additionally, in a
                   FLOPs-matched evaluation, we find that on problems where a
                   smaller base model attains somewhat non-trivial success
                   rates, test-time compute can be used to outperform a 14x
                   larger model.",
  month         =  "6~" # aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.03314",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Kumar2024-fj,
  title         = "Training language models to self-correct via reinforcement
                   learning",
  author        = "Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and
                   Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate
                   and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and
                   Zhang, Lei M and McKinney, Kay and Shrivastava, Disha and
                   Paduraru, Cosmin and Tucker, George and Precup, Doina and
                   Behbahani, Feryal and Faust, Aleksandra",
  journal       = "arXiv [cs.LG]",
  abstract      = "Self-correction is a highly desirable capability of large
                   language models (LLMs), yet it has consistently been found to
                   be largely ineffective in modern LLMs. Current methods for
                   training self-correction typically depend on either multiple
                   models, a more advanced model, or additional forms of
                   supervision. To address these shortcomings, we develop a
                   multi-turn online reinforcement learning (RL) approach,
                   SCoRe, that significantly improves an LLM's self-correction
                   ability using entirely self-generated data. To build SCoRe,
                   we first show that variants of supervised fine-tuning (SFT)
                   on offline model-generated correction traces are often
                   insufficient for instilling self-correction behavior. In
                   particular, we observe that training via SFT falls prey to
                   either a distribution mismatch between mistakes made by the
                   data-collection policy and the model's own responses, or to
                   behavior collapse, where learning implicitly prefers only a
                   certain mode of correction behavior that is often not
                   effective at self-correction on test problems. SCoRe
                   addresses these challenges by training under the model's own
                   distribution of self-generated correction traces and using
                   appropriate regularization to steer the learning process into
                   learning a self-correction behavior that is effective at test
                   time as opposed to fitting high-reward responses for a given
                   prompt. This regularization process includes an initial phase
                   of multi-turn RL on a base model to generate a policy
                   initialization that is less susceptible to collapse, followed
                   by using a reward bonus to amplify self-correction. With
                   Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe
                   achieves state-of-the-art self-correction performance,
                   improving the base models' self-correction by 15.6\% and
                   9.1\% respectively on MATH and HumanEval.",
  month         =  "19~" # sep,
  year          =  2024,
  url           = "http://arxiv.org/abs/2409.12917",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Wu2024-px,
  title         = "Thinking {LLMs}: General instruction following with thought
                   generation",
  author        = "Wu, Tianhao and Lan, Janice and Yuan, Weizhe and Jiao,
                   Jiantao and Weston, Jason and Sukhbaatar, Sainbayar",
  journal       = "arXiv [cs.CL]",
  abstract      = "LLMs are typically trained to answer user questions or follow
                   instructions similarly to how human experts respond. However,
                   in the standard alignment framework they lack the basic
                   ability of explicit thinking before answering. Thinking is
                   important for complex questions that require reasoning and
                   planning -- but can be applied to any task. We propose a
                   training method for equipping existing LLMs with such
                   thinking abilities for general instruction following without
                   use of additional human data. We achieve this by an iterative
                   search and optimization procedure that explores the space of
                   possible thought generations, allowing the model to learn how
                   to think without direct supervision. For each instruction,
                   the thought candidates are scored using a judge model to
                   evaluate their responses only, and then optimized via
                   preference optimization. We show that this procedure leads to
                   superior performance on AlpacaEval and Arena-Hard, and shows
                   gains from thinking on non-reasoning categories such as
                   marketing, health and general knowledge, in addition to more
                   traditional reasoning \& problem-solving tasks.",
  month         =  "14~" # oct,
  year          =  2024,
  url           = "http://arxiv.org/abs/2410.10630",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  keywords      = "o1"
}
